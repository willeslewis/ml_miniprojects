{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background:\n",
    "- This notebook is just for playing around a bit with tensorflow to retrain that muscle memory I had from working with TF at The Data Incubator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of sessions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#create variables in tensorflow\n",
    "x = tf.Variable(3,name='x')\n",
    "y = tf.Variable(4,name='y')\n",
    "\n",
    "#create the computation graph\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "#create a tf session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#initialize session variables\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "#close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can avoid repeating sess.run:\n",
    "with tf.Session() as sess:\n",
    "    #with block sets sess as default session\n",
    "    \n",
    "    #below becomes equivalent to tf.get_default_session().run(x.initializer)\n",
    "    x.initializer.run()  \n",
    "    y.initializer.run()\n",
    "    #below becomes equivalent to tf.get_default_session().run(f)\n",
    "    result = f.eval()\n",
    "#session is automatically closed at end of with block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use global variables initializer instead of doing all by hand\n",
    "init = tf.global_variables_initializer() #prep an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init.run()\n",
    "    results = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#alternately can use an interactive session so we don't need a with block, but we need to manually close the session\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add node to default graph (this is automatic behavior)\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but we can make and manage independent graphs\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can reset the default graph to clean it up\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#Node values are dropped between graph runs\n",
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(y.eval()) #tf sees it need x and hence w so it first evaluates w then x then evaluate y\n",
    "    print(z.eval()) #tf needs x and w again, and will reevaluate w then x then evalute z. x and w will be evaluated 2x\n",
    "    \n",
    "#All node values are dropped between graph runs except variable values which are maintained across graphs (constants \n",
    "#are not variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#we can evaluate y and z in the same graph though:\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)\n",
    "    \n",
    "#Variable states in single process tensor flow are not shared across multiple sessions even with the same graph.\n",
    "#each graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation examples\n",
    "## Solve normal equations:\n",
    "given a linear model $y(\\mathbf{x},\\mathbf{\\theta}) = \\mathbf{x}\\cdot\\mathbf{\\theta}$ where $\\mathbf{\\theta} = [\\theta_1, \\theta_2, ... , \\theta_n]$ and $\\mathbf{x} = [x_1, x_2,... ,x_n]$ is a feature vector of length n. Given a dataset with k observations and n features organzied into a matrix $X \\in \\mathbb{R}^{kxn}$ and k target values $\\mathbf{y}_{target}$ supoose we wish to find $\\hat{\\theta}$ which minimizes the mean square error given by\n",
    "\n",
    "$\\begin{equation}\n",
    "MSE(\\theta) \\propto \\sum_{i = 1} ^k ||\\mathbf{y}(X,\\theta) - \\mathbf{y}_{target}|| ^2\n",
    "\\end{equation}$\n",
    "\n",
    "where $\\mathbf{y}(X,\\theta) = X \\cdot \\theta$ is a matrix of shape $kx1$. Let's minimize the MSE with respect to $\\theta$\n",
    "\n",
    "$\\begin{equation}\n",
    "||\\mathbf{y}(X,\\theta) - \\mathbf{y}_{target}|| * \\partial_\\theta{\\mathbf{y}(X,\\mathbf{\\theta})} = 0\n",
    "\\end{equation}$\n",
    "\n",
    "so\n",
    "\n",
    "$\\begin{equation}\n",
    "||\\mathbf{y}(X,\\theta) - \\mathbf{y}_{target}|| = 0\n",
    "\\end{equation}$\n",
    "\n",
    "or\n",
    "\n",
    "$X\\mathbf{\\theta} = \\mathbf{y}_{target} \\rightarrow X^tX\\mathbf{\\theta} = X^t\\mathbf{y}_{target}\n",
    "\\rightarrow \\mathbf{\\theta} = (X^tX)^{-1}X^t\\mathbf{y}_{target}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch gradient descent\n",
    "This is a numerical approach to minimizing a cost function like MSE. For MSE of the linear regression problem the gradient can be found analytically as\n",
    "\n",
    "$\\begin{equation}\n",
    "\\nabla_\\theta MSE(\\theta) = \\frac{2}{k} X^t(X\\mathbf{\\theta} - \\mathbf{y}_{target})\n",
    "\\end{equation}$\n",
    "\n",
    "If we take a small step opposite the direction of the gradient, we move decrease the MSE.\n",
    "\n",
    "$\\theta^{(i+1)} = \\theta^{(i)} - \\eta \\nabla_\\theta MSE(\\theta)$\n",
    "\n",
    "$\\eta$ here is the learning rate and determines the step size in proportion to the gradient size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 6.0908103\n",
      "Epoch 100 MSE= 0.77572846\n",
      "Epoch 200 MSE= 0.62502444\n",
      "Epoch 300 MSE= 0.59593636\n",
      "Epoch 400 MSE= 0.5784452\n",
      "Epoch 500 MSE= 0.5656278\n",
      "Epoch 600 MSE= 0.556019\n",
      "Epoch 700 MSE= 0.5487716\n",
      "Epoch 800 MSE= 0.5432795\n",
      "Epoch 900 MSE= 0.5390978\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)),scaled_housing]\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype= tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X),error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            print('Epoch', epoch, 'MSE=',mse.eval())\n",
    "            \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8826072 ],\n",
       "       [ 0.15565088],\n",
       "       [-0.31520402],\n",
       "       [ 0.32459393],\n",
       "       [ 0.00823164],\n",
       "       [-0.04357724],\n",
       "       [-0.58113176],\n",
       "       [-0.55551445]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 13.762668\n",
      "Epoch 100 MSE= 0.81799835\n",
      "Epoch 200 MSE= 0.6006053\n",
      "Epoch 300 MSE= 0.5770048\n",
      "Epoch 400 MSE= 0.56378\n",
      "Epoch 500 MSE= 0.5541626\n",
      "Epoch 600 MSE= 0.5470061\n",
      "Epoch 700 MSE= 0.54165065\n",
      "Epoch 800 MSE= 0.5376267\n",
      "Epoch 900 MSE= 0.53458995\n"
     ]
    }
   ],
   "source": [
    "#Let's implement it pretending we didn't know the form of the gradient \n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)),scaled_housing]\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype= tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "gradients = tf.gradients(mse,[theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            print('Epoch', epoch, 'MSE=',mse.eval())\n",
    "            \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above gradient method implements Reverse-mode autodiff \n",
    "- Reverse mode auto diff requires $n_{outputs} + 1$ graph traversals gives high accuracy and supports arbitrary code for functions.\n",
    "\n",
    "Reverse autodiff proceedure:\n",
    "\n",
    "1) Traverse the graph in the forward direction to calculate the node values.\n",
    "\n",
    "2) For each output value, traverse the graph backwards making use of the chain rule to build up the derivatives with respect to the inputs via the chain rule. \n",
    "\n",
    "## Example:\n",
    "\n",
    "Suppose $f(x,y) = x^2y + y + 2$ let's consider calculating the derivative at $f(3,4)$. Our graph would have a multiplication node that takes the input node x (value $ = 3$ node $= n_1$) twice resulting in (value $= 9$ node $= n_4$) and combines that output at another node with input y (value $= 4$ node $ = n_2$) to obtain (value $= 36$ node $= n_5$). $n_2$ also combines with a constant node (value $= 2$ node $ = n_3$) at an addition node to obtain (value $= 6$ node $=n_6$). Finally $n_5$ and $n_6$ are combined in an addition node (value $= 42$ node $= n_7$) to give the function $f(x,y)$. Note the values were optained with a single forward pass through the graph.\n",
    "\n",
    "We may compute the partial derivative with respect to x as follows:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\partial_x f(x,y) = \\partial_{n_7} f(x,y) \\partial_{x} n_7 =  \\partial_{x} n_7\\\\ \n",
    "f(x,y) = n_7\\\\\n",
    "\\partial_{x} n_7 = \\partial_{n_5} n_7 \\partial_{x} n_5 + \\partial_{n_6} n_7 \\partial_{x} n_6 =  \\partial_{x} n_5 +  \\partial_{x} n_6\\\\\n",
    "n_7 = n_5 + n_6\\\\\n",
    "\\partial_{x} n_6 = \\partial_{n_2} n_6 \\partial_{x} n_2 + \\partial_{n_3} n_6 \\partial_{x} n_3= \\partial_{x} n_2 + \\partial_{x} n_3\\\\\n",
    "n_6 = n_2 + n_3\\\\\n",
    "\\partial_{x} n_3 = 0\\\\\n",
    "n_3 = 2\\\\\n",
    "\\partial_{x} n_2 = 0\\\\\n",
    "n_2 = y\\\\\n",
    "\\partial_{x} n_5 = \\partial_{n_2} n_5 \\partial_{x} n_2 + \\partial_{n_4} n_5 \\partial_{x} n_4= n_4 \\partial_{x} n_2 + n_2 \\partial_{x} n_4\\\\\n",
    "n_5 = n_2 n_4\\\\\n",
    "\\partial_{x} n_4 = \\partial_{n_1} n_4 \\partial_{x} n_1 = 2 n_1 \\partial_{x} n_1 = 6\\\\\n",
    "n_1 = x \\rightarrow 3\\\\\n",
    "\\partial_{x} n_2 = 0\\\\\n",
    "n_2 = y\\\\\n",
    "\\end{equation}$\n",
    "\n",
    "So $\\partial_x f(x,y)|_{x=3,y=4} = 1 *(4*6) = 24$.\n",
    "\n",
    "- This approach is very flexible and accurate and can even handle functions not differentiable everywhere as long as we try to compute it at a differentiable point of the function.\n",
    "- Note: if implementing a new operation in tensorflow, in order to make it compatible with autodiff, you must provide a function that builds a subgraph to compute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "- TF also includes some out of the box optimizers so that we don't have to write e.g. our own gradient descent\n",
    "- all we have to do is swap out the optimizer. (Note I use momentum optimizer below, and it converges a bit faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE 10.774582\n",
      "Epoch 100 MSE 0.5247632\n",
      "Epoch 200 MSE 0.52432555\n",
      "Epoch 300 MSE 0.5243211\n",
      "Epoch 400 MSE 0.524321\n",
      "Epoch 500 MSE 0.52432096\n",
      "Epoch 600 MSE 0.52432096\n",
      "Epoch 700 MSE 0.5243209\n",
      "Epoch 800 MSE 0.5243209\n",
      "Epoch 900 MSE 0.5243209\n"
     ]
    }
   ],
   "source": [
    "#using an optimizer instead of our own gradient descent\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            print('Epoch', epoch, 'MSE', mse.eval())\n",
    "            \n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder example\n",
    "A = tf.placeholder(tf.float32, shape = (None,3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    B_val_1 = B.eval(feed_dict = {A:[[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict = {A:[[4,5,6],[7,8,9]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement minibatch \n",
    "works the same as batch gradient descent but uses only a random subset of the data for each batch. Runs many small batches over fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n+1), name = 'X')\n",
    "y = tf.placeholder(tf.float32, shape = (None,1),name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0, seed=42), name = 'theta')\n",
    "y_pred = tf.matmul(X,theta, name = 'predictions')\n",
    "error = y - y_pred\n",
    "mse = tf.reduce_mean(tf.square(error, name = 'mse'))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    \n",
    "    np.random.seed(epoch*batch_index)\n",
    "    \n",
    "    idx = np.random.randint(m,size = batch_size)\n",
    "    \n",
    "    X_batch = scaled_housing_data_plus_bias[idx]\n",
    "    y_batch = housing.target[idx].reshape(-1,1)\n",
    "\n",
    "    return X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #if epoch % 100 == 0:\n",
    "            \n",
    "        #        print('Epoch', epoch, 'MSE', mse.eval())\n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "                  \n",
    "            X_batch, y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op,feed_dict = {X: X_batch, y: y_batch})\n",
    "    \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9045429 ],\n",
       "       [ 0.35481548],\n",
       "       [ 0.5906365 ],\n",
       "       [ 0.51156354],\n",
       "       [-0.04808879],\n",
       "       [ 0.26202965],\n",
       "       [-0.62795925],\n",
       "       [-0.7713845 ],\n",
       "       [-0.32755637]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save by creating a saver node at the end of the construction phase.\n",
    "# Then call the save method in the session when we want to save the model.\n",
    "\n",
    "batch_size =100\n",
    "n_epochs = 10\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape = (None,n+1),name = 'X')\n",
    "y = tf.placeholder(tf.float32,shape = (None,1), name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0, seed = 42),name = 'theta')\n",
    "y_pred = tf.matmul(X,theta)\n",
    "error = y-y_pred\n",
    "mse = tf.reduce_mean(tf.square(error),name = 'mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            X_batch, y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            \n",
    "            sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "        \n",
    "        if epoch % 100 == 2:\n",
    "            \n",
    "            save_path = saver.save(sess,'./tmp/my_model.ckpt')\n",
    "            \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, './tmp/my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0802064 ],\n",
       "       [ 0.81800103],\n",
       "       [ 0.12630619],\n",
       "       [-0.2501979 ],\n",
       "       [ 0.2781734 ],\n",
       "       [-0.01142241],\n",
       "       [-0.01247775],\n",
       "       [-0.906074  ],\n",
       "       [-0.8746359 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Lets load the model back in\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess,'./tmp/my_model_final.ckpt')\n",
    "    \n",
    "    loaded_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0802064 ],\n",
       "       [ 0.81800103],\n",
       "       [ 0.12630619],\n",
       "       [-0.2501979 ],\n",
       "       [ 0.2781734 ],\n",
       "       [-0.01142241],\n",
       "       [-0.01247775],\n",
       "       [-0.906074  ],\n",
       "       [-0.8746359 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = './tf_logs'\n",
    "logdir = '{}/run-{}'.format(root_logdir,now)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n+1),name='X')\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed = 42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "error = y - y_pred\n",
    "mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(mse)\n",
    "mse_summary = tf.summary.scalar('MSE',mse)\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            X_batch, y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            \n",
    "            if batch_index % 10 == 0:\n",
    "                \n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                \n",
    "                step = epoch*n_batches + batch_index\n",
    "                \n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            \n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using name scopes to declutter graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = './tf_logs'\n",
    "logdir = '{}/run-{}'.format(root_logdir,now)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "n_batches = int(np.ceil(m/batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n+1),name='X')\n",
    "y = tf.placeholder(tf.float32,shape=(None,1),name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1],-1.0,1.0,seed = 42),name='theta')\n",
    "y_pred = tf.matmul(X,theta,name='predictions')\n",
    "with tf.name_scope('loss') as scope:\n",
    "    error = y - y_pred\n",
    "    mse = tf.reduce_mean(tf.square(error),name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(mse)\n",
    "mse_summary = tf.summary.scalar('MSE',mse)\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for batch_index in range(n_batches):\n",
    "            \n",
    "            X_batch, y_batch = fetch_batch(epoch,batch_index,batch_size)\n",
    "            \n",
    "            if batch_index % 10 == 0:\n",
    "                \n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch,y:y_batch})\n",
    "                \n",
    "                step = epoch*n_batches + batch_index\n",
    "                \n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(train_op,feed_dict={X:X_batch,y:y_batch})\n",
    "            \n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = './tf_logs'\n",
    "logdir = '{}/run-{}'.format(root_logdir,now)\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape),name = 'weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X,w),b, name ='z')\n",
    "        return tf.maximum(z,0,name='relu')\n",
    "    \n",
    "n_features = 3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    sess.run(output,feed_dict={X:[[1,2,3]]})\n",
    "    \n",
    "    tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = './tf_logs'\n",
    "logdir = '{}/run-{}'.format(root_logdir,now)\n",
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "\n",
    "    w_shape = (int(X.get_shape()[1]),1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape),name = 'weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X,w),b, name ='z')\n",
    "    return tf.maximum(z,0,name='relu')\n",
    "\n",
    "n_features = 3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name='X')\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    final_output = sess.run(output,feed_dict={X:[[1,2,3]]})\n",
    "    \n",
    "    tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.165289]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
